{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99a834ed-a1c7-44f3-a6be-99d89324593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers huggingface_hub\n",
    "!pip install -q --upgrade accelerate\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "368d4e47-8904-4f57-bd34-b945346aef2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/fsspec/registry.py:294: UserWarning: Your installed version of s3fs is very old and known to cause\n",
      "severe performance issues, see also https://github.com/dask/dask/issues/10276\n",
      "\n",
      "To fix, you should specify a lower version bound on s3fs, or\n",
      "update the current installation.\n",
      "\n",
      "  warnings.warn(s3_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good afternoon, champ, how you holding up? Goo...</td>\n",
       "      <td>Subjective:\\n- Symptoms: Lower back pain, radi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What brings you in here today? Hi, I'm um, I'm...</td>\n",
       "      <td>Subjective:\\n- Presenting with dry cough for 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Do you have any known allergies to medications...</td>\n",
       "      <td>Subjective:\\n- No known allergies to medicatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How may I help you today? Yeah I've had, a fev...</td>\n",
       "      <td>Subjective:\\n- Fever and dry cough started 4 d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It sounds like that you're experiencing some c...</td>\n",
       "      <td>Subjective:\\n- Presenting with chest pain for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>What brings you in? Hi. Uh, I've just had this...</td>\n",
       "      <td>Subjective:\\n- Cough for the past week\\n- Pers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Good morning, ma'am. Oh, good morning, doctor....</td>\n",
       "      <td>Subjective:\\n- Symptoms: Difficulty using stai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Are you aware of any medical problems in your ...</td>\n",
       "      <td>Subjective:\\n- No medical problems reported in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>What's going on with you? What brings you here...</td>\n",
       "      <td>Subjective:\\n- Symptoms: Loose watery stools\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>How are you doing today, ma'am? I'm doing just...</td>\n",
       "      <td>Subjective:\\n- Symptoms: Issues with knees.\\n-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                input  \\\n",
       "0   Good afternoon, champ, how you holding up? Goo...   \n",
       "1   What brings you in here today? Hi, I'm um, I'm...   \n",
       "2   Do you have any known allergies to medications...   \n",
       "3   How may I help you today? Yeah I've had, a fev...   \n",
       "4   It sounds like that you're experiencing some c...   \n",
       "..                                                ...   \n",
       "94  What brings you in? Hi. Uh, I've just had this...   \n",
       "95  Good morning, ma'am. Oh, good morning, doctor....   \n",
       "96  Are you aware of any medical problems in your ...   \n",
       "97  What's going on with you? What brings you here...   \n",
       "98  How are you doing today, ma'am? I'm doing just...   \n",
       "\n",
       "                                               output  \n",
       "0   Subjective:\\n- Symptoms: Lower back pain, radi...  \n",
       "1   Subjective:\\n- Presenting with dry cough for 1...  \n",
       "2   Subjective:\\n- No known allergies to medicatio...  \n",
       "3   Subjective:\\n- Fever and dry cough started 4 d...  \n",
       "4   Subjective:\\n- Presenting with chest pain for ...  \n",
       "..                                                ...  \n",
       "94  Subjective:\\n- Cough for the past week\\n- Pers...  \n",
       "95  Subjective:\\n- Symptoms: Difficulty using stai...  \n",
       "96  Subjective:\\n- No medical problems reported in...  \n",
       "97  Subjective:\\n- Symptoms: Loose watery stools\\n...  \n",
       "98  Subjective:\\n- Symptoms: Issues with knees.\\n-...  \n",
       "\n",
       "[99 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the bucket and file names\n",
    "bucket_name = 'agentsum'  # Replace with your bucket name\n",
    "soap = f's3://{bucket_name}/sample_summary.csv'\n",
    "\n",
    "# Load the files\n",
    "soap = pd.read_csv(soap)\n",
    "\n",
    "# Display the data\n",
    "soap.head(99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de3f0b9c-13f8-43cf-b78e-620643e566ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your Hugging Face token:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'user', 'id': '6644f24fe7ae8316ebf3fee4', 'name': 'LizaPiya', 'fullname': 'Fahmida Liza Piya', 'email': 'lizapiya@udel.edu', 'emailVerified': True, 'canPay': False, 'periodEnd': None, 'isPro': False, 'avatarUrl': '/avatars/d43d60b3eba464c3f9b44c34e43b64d6.svg', 'orgs': [], 'auth': {'type': 'access_token', 'accessToken': {'displayName': 'Clinical Note LLama', 'role': 'write', 'createdAt': '2024-06-03T19:29:07.142Z'}}}\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from huggingface_hub import whoami\n",
    "import getpass\n",
    "\n",
    "# Prompt the user for the Hugging Face token at runtime\n",
    "hf_token = getpass.getpass(\"Enter your Hugging Face token: \")\n",
    "\n",
    "# Log in using the provided token\n",
    "login(token=hf_token)\n",
    "\n",
    "\n",
    "print(whoami(token=hf_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90da5bab-71d3-46f2-9eee-ccfbd90dd150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9917d9a29a0476290a6360082f7d173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3524166d87a4d4c8fdf606bb3b00d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6116978d6744454a7d7d8d6bf864419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a3fb7e8215e435e94b3a62d852f2512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdfcc8f7e64041b9a1b21b6da9599061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e0e52559db4ee9af8e37f093665a9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e4cd80ff1e444facb4cbb4d2f1a9aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6bb73cca4244704844894ababb2e671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0aa5b680bf54162983f289bb8702d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f78eae0a5c6845fe8b60f9133ec34ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and processing SOAP dataset...\n",
      "Total samples to process: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [12:21<00:00,  7.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All results saved to 'llama_3.2_3b_summaries_soap_dataset.csv'\n",
      "\n",
      "Dataset Statistics:\n",
      "Total samples processed: 100\n",
      "Successful generations: 100\n",
      "Failed generations: 0\n",
      "\n",
      "Average token counts:\n",
      "Input tokens: 454.6\n",
      "Target tokens: 157.3\n",
      "Generated summary tokens: 321.6\n",
      "\n",
      "Sample result:\n",
      "Input (first 200 chars): Good afternoon, champ, how you holding up? Good afternoon, Doctor, I have a lot of lower back pain. Oh no, before we begin, how old are you, sir and which hand do you write with? I'm seventy five now....\n",
      "Generated summary: The patient, a 75-year-old male, presents with worsening lower back pain that began on December 3, 1995. He reports radiating pain down his legs, which started 3 days after the initial pain began, and weakness in his legs. He has been treated with antiinflammatories, which provided temporary relief but did not address the underlying cause of his symptoms. The patient has seen multiple doctors, including a primary care physician and an orthopedist, who performed imaging studies (CT and x-rays) but were unable to identify a clear cause for his symptoms. The patient reports numbness in his legs since December 11.\n",
      "\n",
      "Subjective: The patient reports worsening lower back pain that began 10 days ago, radiating pain down his legs, and weakness in his legs. He has been experiencing numbness in his legs since December 11.\n",
      "\n",
      "Objective: The patient's physical examination and medical history reveal a 75-year-old male with a 10-day history of worsening\n",
      "Target summary: Subjective:\n",
      "- Symptoms: Lower back pain, radiating pain down the right leg, then the left leg.\n",
      "- Severity: Severe pain, described as \"I could barely walk\" and \"the pain was so severe.\"\n",
      "- Duration: Lower back pain for about ten days; radiating pain started three days after back pain began (right leg), then left leg three days later.\n",
      "- Associated symptoms: Weakness in the legs, leg numbness since December eleventh.\n",
      "- Relevant medical history: Saw local physician on December eleventh; received anti-inflammatory medications which were ineffective.\n",
      "- Family history: Not mentioned.\n",
      "- Allergies: Not mentioned.\n",
      "- Other concerns: Patient expressed difficulty walking and concerns about the severity of their symptoms.\n",
      "\n",
      "Objective:\n",
      "- Physical examination findings: Not provided.\n",
      "- Diagnostic test results: CT and X-rays of the lower back performed by the orthopedist, but no abnormalities were noted.\n",
      "- Vital signs: Not provided.\n",
      "\n",
      "Assessment:\n",
      "- Diagnoses: Not explicitly stated, but indicated severe lower back pain with radiculopathy.\n",
      "- Differential diagnoses: Not provided.\n",
      "- Clinical impressions: Not explicitly stated.\n",
      "\n",
      "Plan:\n",
      "- Follow up at U I H C as recommended by the orthopedist. \n",
      "- No specific medications, treatments, or further tests mentioned in the conversation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Medical dialogue summarization function\n",
    "def generate_summary(dialogue):\n",
    "    prompt = f\"\"\"You are a medical expert. Analyze the following patient-doctor dialogue and provide a concise medical summary in SOAP format (Subjective, Objective, Assessment, Plan):\\n\\n{dialogue}\\n\\nSummary:\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True).replace(prompt, \"\").strip()\n",
    "\n",
    "# Apply to SOAP dataset\n",
    "def process_soap_data(df):\n",
    "    results = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Generating summaries\"):\n",
    "        try:\n",
    "            summary = generate_summary(row['input'])\n",
    "            summary_tokens = len(tokenizer.encode(summary))\n",
    "            input_tokens = len(tokenizer.encode(row['input']))\n",
    "            target_tokens = len(tokenizer.encode(row['output']))\n",
    "            \n",
    "            results.append({\n",
    "                'sample_id': idx,\n",
    "                'original_input': row['input'],\n",
    "                'generated_summary': summary,\n",
    "                'target_summary': row['output'],\n",
    "                'summary_token_count': summary_tokens,\n",
    "                'input_tokens': input_tokens,\n",
    "                'target_tokens': target_tokens\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample {idx}: {str(e)}\")\n",
    "            results.append({\n",
    "                'sample_id': idx,\n",
    "                'original_input': row['input'],\n",
    "                'generated_summary': \"ERROR: Could not generate summary\",\n",
    "                'target_summary': row['output'],\n",
    "                'summary_token_count': 0,\n",
    "                'input_tokens': len(tokenizer.encode(row['input'])) if pd.notna(row['input']) else 0,\n",
    "                'target_tokens': len(tokenizer.encode(row['output'])) if pd.notna(row['output']) else 0\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Load your SOAP dataset\n",
    "# Replace 'soap' with your actual dataframe variable name\n",
    "print(\"Loading and processing SOAP dataset...\")\n",
    "print(f\"Total samples to process: {len(soap)}\")\n",
    "\n",
    "# Process the dataset\n",
    "full_results = process_soap_data(soap)\n",
    "\n",
    "# Save results\n",
    "output_filename = 'llama_3.2_3b_summaries_soap_dataset.csv'\n",
    "full_results.to_csv(output_filename, index=False)\n",
    "print(f\"\\nAll results saved to '{output_filename}'\")\n",
    "\n",
    "# Display some basic statistics\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"Total samples processed: {len(full_results)}\")\n",
    "print(f\"Successful generations: {len(full_results[full_results['generated_summary'] != 'ERROR: Could not generate summary'])}\")\n",
    "print(f\"Failed generations: {len(full_results[full_results['generated_summary'] == 'ERROR: Could not generate summary'])}\")\n",
    "\n",
    "# Display average token counts\n",
    "successful_results = full_results[full_results['generated_summary'] != 'ERROR: Could not generate summary']\n",
    "if len(successful_results) > 0:\n",
    "    print(f\"\\nAverage token counts:\")\n",
    "    print(f\"Input tokens: {successful_results['input_tokens'].mean():.1f}\")\n",
    "    print(f\"Target tokens: {successful_results['target_tokens'].mean():.1f}\")\n",
    "    print(f\"Generated summary tokens: {successful_results['summary_token_count'].mean():.1f}\")\n",
    "\n",
    "# Show a sample result\n",
    "if len(successful_results) > 0:\n",
    "    print(f\"\\nSample result:\")\n",
    "    sample = successful_results.iloc[0]\n",
    "    print(f\"Input (first 200 chars): {sample['original_input'][:200]}...\")\n",
    "    print(f\"Generated summary: {sample['generated_summary']}\")\n",
    "    print(f\"Target summary: {sample['target_summary']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e0b762-7fd1-4e3e-b807-b79be158af87",
   "metadata": {},
   "source": [
    "### Traditional Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48052a13-425e-43da-9239-849f7fdb805c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q nltk bert-score\n",
    "!pip install -q rouge-metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "051bf981-d848-493d-9019-f91c93c8a3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Llama-3.2-3B SOAP Evaluation\n",
      "================================================================================\n",
      "üìÇ Loaded 100 samples from llama_3.2_3b_summaries_soap_dataset.csv\n",
      "üìã Data columns: ['sample_id', 'original_input', 'generated_summary', 'target_summary', 'summary_token_count', 'input_tokens', 'target_tokens']\n",
      "üìè Data shape: (100, 7)\n",
      "üìä Evaluating 100 Llama-generated summaries...\n",
      "üìù Summary column: generated_summary\n",
      "üéØ Reference column: target_summary\n",
      "\n",
      "üî¢ Computing BLEU and ROUGE-L scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 67.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Computing BERTScore...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä LLAMA-3.2-3B SOAP EVALUATION RESULTS\n",
      "================================================================================\n",
      "Metric       Mean ¬± Std           Min      Max      Median  \n",
      "----------------------------------------------------------------------\n",
      "BLEU1        15.116 ¬± 9.353    1.55     43.71    12.71   \n",
      "BLEU2        8.095 ¬± 6.168    0.43     28.85    6.62    \n",
      "ROUGE_L      12.529 ¬± 5.311    2.48     25.32    12.15   \n",
      "BERT_P       83.447 ¬± 2.922    77.19    88.60    83.96   \n",
      "BERT_R       84.581 ¬± 2.540    78.07    89.14    84.84   \n",
      "BERT_F1      83.986 ¬± 2.338    77.83    88.53    84.20   \n",
      "\n",
      "================================================================================\n",
      "üìà BASELINE METRICS SUMMARY (for table)\n",
      "================================================================================\n",
      "BLEU-1: 15.12 ¬± 9.35\n",
      "BLEU-2: 8.10 ¬± 6.17\n",
      "ROUGE-L: 12.53 ¬± 5.31\n",
      "BERTScore-F1: 83.99 ¬± 2.34\n",
      "\n",
      "================================================================================\n",
      "üìè TOKEN LENGTH ANALYSIS\n",
      "================================================================================\n",
      "Generated Summary Tokens:\n",
      "  Mean ¬± Std: 321.6 ¬± 489.3\n",
      "  Target: 200 tokens\n",
      "  Range: 125 - 2248\n",
      "  Within 180-220: 91/100 (91.0%)\n",
      "\n",
      "Target Summary Tokens:\n",
      "  Mean ¬± Std: 157.3 ¬± 118.3\n",
      "  Range: 37 - 473\n",
      "\n",
      "üíæ Results saved to: llama_3.2_3b_summaries_soap_dataset_evaluation_results.csv\n",
      "\n",
      "================================================================================\n",
      "üìã SAMPLE RESULTS\n",
      "================================================================================\n",
      "Sample ID: 0\n",
      "Generated Summary Token Count: 201\n",
      "BLEU-1: 27.63\n",
      "ROUGE-L: 15.07\n",
      "BERTScore-F1: 86.28\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "# Run the entire script\n",
    "from Soap_llama import run_llama_evaluation\n",
    "\n",
    "# Run evaluation\n",
    "df_results = run_llama_evaluation(\"llama_3.2_3b_summaries_soap_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6ca75b-f24b-4957-8cf7-fc774ce1be31",
   "metadata": {},
   "source": [
    "### Llm_as_a_judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed87a611-42b6-47d7-ab7c-c7b3dfa0314a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing single Llama-3.2-3B SOAP sample...\n",
      "üß™ Testing single Llama-3.2-3B SOAP sample evaluation...\n",
      "Testing sample 0 from 100 total samples\n",
      "Sample ID: 0\n",
      "Summary token count: 201\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c7310de957041829e649d95f64025c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900471dc8bff41c38dba2bfc38cbe2a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "737e4f24603f41dc8a5423c0441a6ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c23d4ad72b40db81ca536e2cec0c31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd1b355e882f4426bb0678349c56d866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062db6261fbb4c7cb11687a4bce1620b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c946b2972b841b3a6b21591d730b688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e4018cde964925b25151b782d060a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90cbd95160f349228c9878f05f75eca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f165817dd9b74c40a61eb4e749890817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dd7ec79b35e4721a791f5dc74c1f587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1363ec26c95644819feb47050c48019c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Source length: 1701 characters\n",
      "üìù Summary length: 949 characters\n",
      "üìÑ Source preview: Good afternoon, champ, how you holding up? Good afternoon, Doctor, I have a lot of lower back pain. Oh no, before we begin, how old are you, sir and which hand do you write with? I'm seventy five now....\n",
      "üìù Summary preview: The patient, a 75-year-old male, presents with worsening lower back pain that began on December 3, 1995. He reports radiating pain down his legs, which started 3 days after the initial pain began, and weakness in his legs. He has been treated with antiinflammatories, which provided temporary relief but did not address the underlying cause of his symptoms. The patient has seen multiple doctors, including a primary care physician and an orthopedist, who performed imaging studies (CT and x-rays) but were unable to identify a clear cause for his symptoms. The patient reports numbness in his legs since December 11.\n",
      "\n",
      "Subjective: The patient reports worsening lower back pain that began 10 days ago, radiating pain down his legs, and weakness in his legs. He has been experiencing numbness in his legs since December 11.\n",
      "\n",
      "Objective: The patient's physical examination and medical history reveal a 75-year-old male with a 10-day history of worsening\n",
      "\n",
      "üìù Prompt length: 3082 characters\n",
      "\n",
      "ü§ñ MODEL RESPONSE:\n",
      "\n",
      "\n",
      "Please rate the generated SOAP summary against the source patient-doctor conversation. \n",
      "\n",
      "Hallucination: 2 (some minor inaccuracies, e.g., the patient's age is not explicitly stated in the conversation)\n",
      "Factual: 4 (most information is accurate, but some minor details are missing or incorrect, e.g., the patient's age, the exact dates of the symptoms)\n",
      "Complete: 3 (some key information is missing, e.g., the patient's occupation, the exact nature\n",
      "\n",
      "üìä EXTRACTED SCORES:\n",
      "Hallucination: 2\n",
      "Factual: 4\n",
      "Complete: 3\n",
      "Coherent: Not found\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Does the test look good? Run full evaluation? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running full Llama-3.2-3B SOAP evaluation...\n",
      "üîÑ Loading Llama 3 8B model as judge...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a62855a888b49ea9c6467985b1f8fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Loaded 100 Llama-3.2-3B generated SOAP summaries\n",
      "üìã Columns: ['sample_id', 'original_input', 'generated_summary', 'target_summary', 'summary_token_count', 'input_tokens', 'target_tokens']\n",
      "üîç Evaluating Llama-3.2-3B SOAP summaries for hallucinations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/100 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:   1%|          | 1/100 [00:06<11:01,  6.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:   2%|‚ñè         | 2/100 [00:12<10:11,  6.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:   3%|‚ñé         | 3/100 [00:18<09:42,  6.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:   4%|‚ñç         | 4/100 [00:20<07:02,  4.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:   5%|‚ñå         | 5/100 [00:22<05:33,  3.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:   6%|‚ñå         | 6/100 [00:25<05:11,  3.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:   7%|‚ñã         | 7/100 [00:28<05:07,  3.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:   8%|‚ñä         | 8/100 [00:30<04:22,  2.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:   9%|‚ñâ         | 9/100 [00:36<05:42,  3.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  10%|‚ñà         | 10/100 [00:39<05:27,  3.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  11%|‚ñà         | 11/100 [00:45<06:20,  4.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  12%|‚ñà‚ñè        | 12/100 [00:46<05:00,  3.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  13%|‚ñà‚ñé        | 13/100 [00:48<04:24,  3.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  14%|‚ñà‚ñç        | 14/100 [00:54<05:30,  3.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  15%|‚ñà‚ñå        | 15/100 [01:00<06:14,  4.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  16%|‚ñà‚ñå        | 16/100 [01:01<04:51,  3.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  17%|‚ñà‚ñã        | 17/100 [01:07<05:44,  4.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  18%|‚ñà‚ñä        | 18/100 [01:10<05:07,  3.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  19%|‚ñà‚ñâ        | 19/100 [01:14<05:10,  3.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  20%|‚ñà‚ñà        | 20/100 [01:18<05:17,  3.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  21%|‚ñà‚ñà        | 21/100 [01:23<05:38,  4.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  22%|‚ñà‚ñà‚ñè       | 22/100 [01:26<05:02,  3.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  23%|‚ñà‚ñà‚ñé       | 23/100 [01:32<05:40,  4.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  24%|‚ñà‚ñà‚ñç       | 24/100 [01:33<04:40,  3.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  25%|‚ñà‚ñà‚ñå       | 25/100 [01:36<04:09,  3.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  26%|‚ñà‚ñà‚ñå       | 26/100 [01:38<03:48,  3.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  27%|‚ñà‚ñà‚ñã       | 27/100 [01:42<03:46,  3.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  28%|‚ñà‚ñà‚ñä       | 28/100 [01:47<04:25,  3.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  29%|‚ñà‚ñà‚ñâ       | 29/100 [01:49<03:47,  3.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  30%|‚ñà‚ñà‚ñà       | 30/100 [01:55<04:37,  3.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  31%|‚ñà‚ñà‚ñà       | 31/100 [01:58<04:20,  3.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/100 [02:03<04:49,  4.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [02:06<04:18,  3.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  34%|‚ñà‚ñà‚ñà‚ñç      | 34/100 [02:12<04:50,  4.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  35%|‚ñà‚ñà‚ñà‚ñå      | 35/100 [02:14<04:10,  3.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/100 [02:20<04:41,  4.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  37%|‚ñà‚ñà‚ñà‚ñã      | 37/100 [02:23<04:12,  4.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/100 [02:26<03:38,  3.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  39%|‚ñà‚ñà‚ñà‚ñâ      | 39/100 [02:27<02:51,  2.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  40%|‚ñà‚ñà‚ñà‚ñà      | 40/100 [02:32<03:40,  3.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [02:36<03:27,  3.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/100 [02:39<03:15,  3.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/100 [02:44<03:52,  4.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 44/100 [02:48<03:35,  3.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 45/100 [02:53<04:02,  4.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/100 [02:57<03:47,  4.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/100 [03:03<04:07,  4.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/100 [03:09<04:18,  4.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [03:12<03:51,  4.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 50/100 [03:17<04:00,  4.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/100 [03:23<04:08,  5.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/100 [03:28<04:00,  5.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 53/100 [03:34<04:05,  5.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 54/100 [03:39<04:07,  5.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 55/100 [03:45<04:06,  5.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/100 [03:51<04:03,  5.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [03:57<04:00,  5.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 58/100 [03:58<03:00,  4.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 59/100 [04:04<03:13,  4.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 60/100 [04:06<02:36,  3.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/100 [04:09<02:20,  3.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/100 [04:10<01:54,  3.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 63/100 [04:13<01:45,  2.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 64/100 [04:17<01:54,  3.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [04:20<01:51,  3.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/100 [04:25<02:14,  3.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/100 [04:31<02:27,  4.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 68/100 [04:37<02:35,  4.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 69/100 [04:43<02:38,  5.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 70/100 [04:44<02:01,  4.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/100 [04:50<02:12,  4.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/100 [04:56<02:19,  4.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [05:02<02:20,  5.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 74/100 [05:04<01:50,  4.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 75/100 [05:09<01:54,  4.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/100 [05:11<01:28,  3.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 77/100 [05:16<01:38,  4.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 78/100 [05:21<01:35,  4.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 79/100 [05:23<01:19,  3.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 80/100 [05:27<01:18,  3.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [05:32<01:15,  3.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 82/100 [05:37<01:20,  4.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 83/100 [05:42<01:17,  4.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 84/100 [05:48<01:18,  4.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 85/100 [05:53<01:17,  5.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/100 [05:59<01:14,  5.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 87/100 [06:04<01:08,  5.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 88/100 [06:10<01:04,  5.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [06:16<01:00,  5.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 90/100 [06:22<00:55,  5.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/100 [06:27<00:50,  5.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 92/100 [06:33<00:45,  5.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 93/100 [06:35<00:32,  4.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 94/100 [06:38<00:23,  3.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 95/100 [06:43<00:21,  4.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/100 [06:48<00:18,  4.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [06:51<00:12,  4.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 98/100 [06:57<00:09,  4.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 99/100 [07:02<00:04,  4.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [07:08<00:00,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä LLAMA-3.2-3B SOAP HALLUCINATION EVALUATION RESULTS:\n",
      "======================================================================\n",
      "Metric                    Mean ¬± Std      Min    Max    Perfect Scores\n",
      "----------------------------------------------------------------------\n",
      "Hallucination (1-5)       2.20 ¬± 0.51   1.0    3.0    5/100\n",
      "Factual Consistency (1-5) 3.84 ¬± 0.37   3.0    4.0    0/100\n",
      "Completeness (1-5)        3.14 ¬± 0.47   2.0    5.0    2/100\n",
      "Coherence (1-5)           4.38 ¬± 0.80   3.0    5.0    58/100\n",
      "\n",
      "üìã BASELINE QUALITY INSIGHTS:\n",
      "‚Ä¢ High hallucination (‚â•4): 0/100 (0.0%)\n",
      "‚Ä¢ Low factual consistency (‚â§2): 0/100 (0.0%)\n",
      "‚Ä¢ Good completeness (‚â•4): 15/100 (15.0%)\n",
      "‚Ä¢ Good coherence (‚â•4): 80/100 (80.0%)\n",
      "\n",
      "üìä FOR BASELINE TABLE:\n",
      "Hallucination: 2.20 ¬± 0.51\n",
      "Factual Consistency: 3.84 ¬± 0.37\n",
      "Completeness: 3.14 ¬± 0.47\n",
      "Coherence: 4.38 ¬± 0.80\n",
      "\n",
      "üíæ Results saved to: llama_3.2_3b_soap_judge_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "# Run the entire script\n",
    "%run llm_as_a_judge_llama.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ea833e-3a9f-4262-956b-5e5d7f0b973f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
