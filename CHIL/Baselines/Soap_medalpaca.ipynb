{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f0a7fda-71a6-46a9-81b8-16660d5fab4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers huggingface_hub\n",
    "!pip install -q --upgrade accelerate\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "020f5416-99f0-4f08-b277-af882873984b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good afternoon, champ, how you holding up? Goo...</td>\n",
       "      <td>Subjective:\\n- Symptoms: Lower back pain, radi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What brings you in here today? Hi, I'm um, I'm...</td>\n",
       "      <td>Subjective:\\n- Presenting with dry cough for 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Do you have any known allergies to medications...</td>\n",
       "      <td>Subjective:\\n- No known allergies to medicatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How may I help you today? Yeah I've had, a fev...</td>\n",
       "      <td>Subjective:\\n- Fever and dry cough started 4 d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It sounds like that you're experiencing some c...</td>\n",
       "      <td>Subjective:\\n- Presenting with chest pain for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>What brings you in? Hi. Uh, I've just had this...</td>\n",
       "      <td>Subjective:\\n- Cough for the past week\\n- Pers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Good morning, ma'am. Oh, good morning, doctor....</td>\n",
       "      <td>Subjective:\\n- Symptoms: Difficulty using stai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Are you aware of any medical problems in your ...</td>\n",
       "      <td>Subjective:\\n- No medical problems reported in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>What's going on with you? What brings you here...</td>\n",
       "      <td>Subjective:\\n- Symptoms: Loose watery stools\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>How are you doing today, ma'am? I'm doing just...</td>\n",
       "      <td>Subjective:\\n- Symptoms: Issues with knees.\\n-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                input  \\\n",
       "0   Good afternoon, champ, how you holding up? Goo...   \n",
       "1   What brings you in here today? Hi, I'm um, I'm...   \n",
       "2   Do you have any known allergies to medications...   \n",
       "3   How may I help you today? Yeah I've had, a fev...   \n",
       "4   It sounds like that you're experiencing some c...   \n",
       "..                                                ...   \n",
       "94  What brings you in? Hi. Uh, I've just had this...   \n",
       "95  Good morning, ma'am. Oh, good morning, doctor....   \n",
       "96  Are you aware of any medical problems in your ...   \n",
       "97  What's going on with you? What brings you here...   \n",
       "98  How are you doing today, ma'am? I'm doing just...   \n",
       "\n",
       "                                               output  \n",
       "0   Subjective:\\n- Symptoms: Lower back pain, radi...  \n",
       "1   Subjective:\\n- Presenting with dry cough for 1...  \n",
       "2   Subjective:\\n- No known allergies to medicatio...  \n",
       "3   Subjective:\\n- Fever and dry cough started 4 d...  \n",
       "4   Subjective:\\n- Presenting with chest pain for ...  \n",
       "..                                                ...  \n",
       "94  Subjective:\\n- Cough for the past week\\n- Pers...  \n",
       "95  Subjective:\\n- Symptoms: Difficulty using stai...  \n",
       "96  Subjective:\\n- No medical problems reported in...  \n",
       "97  Subjective:\\n- Symptoms: Loose watery stools\\n...  \n",
       "98  Subjective:\\n- Symptoms: Issues with knees.\\n-...  \n",
       "\n",
       "[99 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the bucket and file names\n",
    "bucket_name = 'agentsum'  # Replace with your bucket name\n",
    "soap = f's3://{bucket_name}/sample_summary.csv'\n",
    "\n",
    "# Load the files\n",
    "soap = pd.read_csv(soap)\n",
    "\n",
    "# Display the data\n",
    "soap.head(99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95ada422-9823-4ac1-9d3b-d062fac478f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your Hugging Face token:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'user', 'id': '6644f24fe7ae8316ebf3fee4', 'name': 'LizaPiya', 'fullname': 'Fahmida Liza Piya', 'email': 'lizapiya@udel.edu', 'emailVerified': True, 'canPay': False, 'periodEnd': None, 'isPro': False, 'avatarUrl': '/avatars/d43d60b3eba464c3f9b44c34e43b64d6.svg', 'orgs': [], 'auth': {'type': 'access_token', 'accessToken': {'displayName': 'Clinical Note LLama', 'role': 'write', 'createdAt': '2024-06-03T19:29:07.142Z'}}}\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from huggingface_hub import whoami\n",
    "import getpass\n",
    "\n",
    "# Prompt the user for the Hugging Face token at runtime\n",
    "hf_token = getpass.getpass(\"Enter your Hugging Face token: \")\n",
    "\n",
    "# Log in using the provided token\n",
    "login(token=hf_token)\n",
    "\n",
    "\n",
    "print(whoami(token=hf_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c017630-f40d-4585-8c0f-79e3108909fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22b7e78b-f3c7-487a-9894-4409cc421c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['pad_token_id']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['pad_token_id']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "925118a07c2e440fac15b6b8f3de8bc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and processing SOAP dataset...\n",
      "Total samples to process: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries:   1%|          | 1/100 [00:19<32:55, 19.95s/it]This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2248 > 512). Running this sequence through the model will result in indexing errors\n",
      "Generating summaries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [05:58<00:00,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All results saved to 'medalpaca_summaries_soap_dataset.csv'\n",
      "\n",
      "Dataset Statistics:\n",
      "Total samples processed: 100\n",
      "Successful generations: 100\n",
      "Failed generations: 0\n",
      "\n",
      "Average token counts:\n",
      "Input tokens: 503.4\n",
      "Target tokens: 201.1\n",
      "Generated summary tokens: 295.5\n",
      "\n",
      "Sample result:\n",
      "Input (first 200 chars): Good afternoon, champ, how you holding up? Good afternoon, Doctor, I have a lot of lower back pain. Oh no, before we begin, how old are you, sir and which hand do you write with? I'm seventy five now....\n",
      "Generated summary: This patient has had chronic lower back pain for 10 days, which is worsening, with radiating pain to both legs. His symptoms began after he was diagnosed with aortic stenosis, which may be contributing to his pain.\n",
      "Target summary: Subjective:\n",
      "- Symptoms: Lower back pain, radiating pain down the right leg, then the left leg.\n",
      "- Severity: Severe pain, described as \"I could barely walk\" and \"the pain was so severe.\"\n",
      "- Duration: Lower back pain for about ten days; radiating pain started three days after back pain began (right leg), then left leg three days later.\n",
      "- Associated symptoms: Weakness in the legs, leg numbness since December eleventh.\n",
      "- Relevant medical history: Saw local physician on December eleventh; received anti-inflammatory medications which were ineffective.\n",
      "- Family history: Not mentioned.\n",
      "- Allergies: Not mentioned.\n",
      "- Other concerns: Patient expressed difficulty walking and concerns about the severity of their symptoms.\n",
      "\n",
      "Objective:\n",
      "- Physical examination findings: Not provided.\n",
      "- Diagnostic test results: CT and X-rays of the lower back performed by the orthopedist, but no abnormalities were noted.\n",
      "- Vital signs: Not provided.\n",
      "\n",
      "Assessment:\n",
      "- Diagnoses: Not explicitly stated, but indicated severe lower back pain with radiculopathy.\n",
      "- Differential diagnoses: Not provided.\n",
      "- Clinical impressions: Not explicitly stated.\n",
      "\n",
      "Plan:\n",
      "- Follow up at U I H C as recommended by the orthopedist. \n",
      "- No specific medications, treatments, or further tests mentioned in the conversation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load model with safetensors\n",
    "model_name = \"medalpaca/medalpaca-7b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    use_safetensors=True\n",
    ")\n",
    "\n",
    "# Set padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Medical dialogue summarization function - CHANGE 1: Updated for SOAP dialogue\n",
    "def generate_summary(dialogue):\n",
    "    # CHANGE 2: Updated prompt for medical dialogue and SOAP format\n",
    "    prompt = (\n",
    "        \"### Instruction:\\n\"\n",
    "        \"You are a medical summarization expert. Analyze the following patient-doctor dialogue and provide a concise medical summary in SOAP format (Subjective, Objective, Assessment, Plan) in no more than 150 words:\\n\\n\"\n",
    "        f\"{dialogue}\\n\\n\"\n",
    "        \"### Response:\"\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,  # Keep same token count\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True).replace(prompt, \"\").strip()\n",
    "\n",
    "# Apply to SOAP dataset - CHANGE 3: Updated function name and structure\n",
    "def process_soap_data(df):\n",
    "    results = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Generating summaries\"):\n",
    "        try:\n",
    "            summary = generate_summary(row['input'])\n",
    "            summary_tokens = len(tokenizer.encode(summary))\n",
    "            input_tokens = len(tokenizer.encode(row['input']))\n",
    "            target_tokens = len(tokenizer.encode(row['output']))  # CHANGE 4: 'output' instead of 'target'\n",
    "            \n",
    "            results.append({\n",
    "                'sample_id': idx,  # CHANGE 5: sample_id instead of note_id\n",
    "                'original_input': row['input'],\n",
    "                'generated_summary': summary,\n",
    "                'target_summary': row['output'],  # CHANGE 6: 'output' column from SOAP\n",
    "                'summary_token_count': summary_tokens,\n",
    "                'input_tokens': input_tokens,\n",
    "                'target_tokens': target_tokens\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample {idx}: {str(e)}\")\n",
    "            results.append({\n",
    "                'sample_id': idx,\n",
    "                'original_input': row['input'],\n",
    "                'generated_summary': f\"ERROR: {str(e)}\",\n",
    "                'target_summary': row['output'],\n",
    "                'summary_token_count': 0,\n",
    "                'input_tokens': len(tokenizer.encode(row['input'])) if pd.notna(row['input']) else 0,\n",
    "                'target_tokens': len(tokenizer.encode(row['output'])) if pd.notna(row['output']) else 0\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run the pipeline - CHANGE 7: Updated for SOAP dataset\n",
    "print(\"Loading and processing SOAP dataset...\")\n",
    "print(f\"Total samples to process: {len(soap)}\")  # CHANGE 8: 'soap' instead of 'mimic_iv_bhc_100'\n",
    "\n",
    "full_results = process_soap_data(soap)  # CHANGE 9: Updated function call\n",
    "\n",
    "# Save results - CHANGE 10: Update output filename for SOAP\n",
    "output_path = 'medalpaca_summaries_soap_dataset.csv'\n",
    "full_results.to_csv(output_path, index=False)\n",
    "print(f\"\\nAll results saved to '{output_path}'\")\n",
    "\n",
    "# Display some basic statistics\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"Total samples processed: {len(full_results)}\")\n",
    "print(f\"Successful generations: {len(full_results[~full_results['generated_summary'].str.startswith('ERROR:')])}\")\n",
    "print(f\"Failed generations: {len(full_results[full_results['generated_summary'].str.startswith('ERROR:')])}\")\n",
    "\n",
    "# Display average token counts\n",
    "successful_results = full_results[~full_results['generated_summary'].str.startswith('ERROR:')]\n",
    "if len(successful_results) > 0:\n",
    "    print(f\"\\nAverage token counts:\")\n",
    "    print(f\"Input tokens: {successful_results['input_tokens'].mean():.1f}\")\n",
    "    print(f\"Target tokens: {successful_results['target_tokens'].mean():.1f}\")\n",
    "    print(f\"Generated summary tokens: {successful_results['summary_token_count'].mean():.1f}\")\n",
    "\n",
    "# Show a sample result\n",
    "if len(successful_results) > 0:\n",
    "    print(f\"\\nSample result:\")\n",
    "    sample = successful_results.iloc[0]\n",
    "    print(f\"Input (first 200 chars): {sample['original_input'][:200]}...\")\n",
    "    print(f\"Generated summary: {sample['generated_summary']}\")\n",
    "    print(f\"Target summary: {sample['target_summary']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f636dea-5caa-4261-96e1-a2929eca449f",
   "metadata": {},
   "source": [
    "### Traditional Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a6a189e-f65c-46bc-8265-c5a583242bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q nltk bert-score\n",
    "!pip install -q rouge-metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b808f44-f8cc-4720-8afe-a4aaf667e138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting MedAlpaca-7B SOAP Evaluation\n",
      "================================================================================\n",
      "üìÇ Loaded 100 samples from medalpaca_summaries_soap_dataset.csv\n",
      "üìã Data columns: ['sample_id', 'original_input', 'generated_summary', 'target_summary', 'summary_token_count', 'input_tokens', 'target_tokens']\n",
      "üìè Data shape: (100, 7)\n",
      "üìä Evaluating 100 MedAlpaca-generated summaries...\n",
      "üìù Summary column: generated_summary\n",
      "üéØ Reference column: target_summary\n",
      "\n",
      "üî¢ Computing BLEU and ROUGE-L scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01<00:00, 60.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Computing BERTScore...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e39d95fc02d4ddf8658e0522ca67af7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d03066c3364b43bd7485a1d09af9d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "818236ec89b9484c86457bd1906c6e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c45ce4e61b40d4a752570e8e94b15c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "051acf8752ed447f925f1021f2895006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6ba211dab5f4a0f9bbc2c37a53d0410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä MEDALPACA-7B SOAP EVALUATION RESULTS\n",
      "================================================================================\n",
      "Metric       Mean ¬± Std           Min      Max      Median  \n",
      "----------------------------------------------------------------------\n",
      "BLEU1        14.413 ¬± 9.870    0.00     39.98    13.98   \n",
      "BLEU2        7.160 ¬± 6.235    0.00     26.93    5.21    \n",
      "ROUGE_L      15.754 ¬± 9.195    0.00     42.42    13.88   \n",
      "BERT_P       86.726 ¬± 4.026    77.33    94.56    87.10   \n",
      "BERT_R       83.405 ¬± 3.336    72.82    91.43    83.97   \n",
      "BERT_F1      85.008 ¬± 3.391    77.22    92.86    85.56   \n",
      "\n",
      "================================================================================\n",
      "üìà BASELINE METRICS SUMMARY (for table)\n",
      "================================================================================\n",
      "BLEU-1: 14.41 ¬± 9.87\n",
      "BLEU-2: 7.16 ¬± 6.23\n",
      "ROUGE-L: 15.75 ¬± 9.20\n",
      "BERTScore-F1: 85.01 ¬± 3.39\n",
      "\n",
      "================================================================================\n",
      "üìè TOKEN LENGTH ANALYSIS\n",
      "================================================================================\n",
      "Generated Summary Tokens:\n",
      "  Mean ¬± Std: 295.5 ¬± 641.3\n",
      "  Target: 150 tokens\n",
      "  Range: 4 - 2248\n",
      "  Within 130-170: 6/100 (6.0%)\n",
      "\n",
      "Target Summary Tokens:\n",
      "  Mean ¬± Std: 201.1 ¬± 145.7\n",
      "  Range: 51 - 589\n",
      "\n",
      "üíæ Results saved to: medalpaca_summaries_soap_dataset_evaluation_results.csv\n",
      "\n",
      "================================================================================\n",
      "üìã SAMPLE RESULTS\n",
      "================================================================================\n",
      "Sample ID: 0\n",
      "Generated Summary Token Count: 56\n",
      "BLEU-1: 0.48\n",
      "ROUGE-L: 6.96\n",
      "BERTScore-F1: 84.56\n",
      "\n",
      "üéâ MedAlpaca evaluation completed!\n",
      "\n",
      "üìä Use these results as baseline metrics for your comparison table!\n"
     ]
    }
   ],
   "source": [
    "%run medalpaca.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30a92b50-3d2f-4f50-9a82-ba632b985ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing single MedAlpaca SOAP sample...\n",
      "üß™ Testing single MedAlpaca SOAP sample evaluation...\n",
      "Testing sample 0 from 100 total samples\n",
      "Sample ID: 0\n",
      "Summary token count: 56\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "711a1e58752d4a29b1d1fb63ce027ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf262c1321d848b296429c687e59e253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2455fcf1c00488682c992b8862c7df2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea41d4e396f481191dc8ec0f9dcc2e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0947e7caa22b403aa7d33da2aab3a4e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1aa2461a07f46f58fbd80c926d81cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1125b87a9b66413b873aab62e6569c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e78ded450d4e06bd79fe782b169c62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deff84216b184ee6958314ed5fc729f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "091420d2717b417a9b792c48fd61040c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d66459b84ed46a8b04f6d6866f76c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0890328aee7f47f9997f76460c5bccf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Source length: 1701 characters\n",
      "üìù Summary length: 214 characters\n",
      "üìÑ Source preview: Good afternoon, champ, how you holding up? Good afternoon, Doctor, I have a lot of lower back pain. Oh no, before we begin, how old are you, sir and which hand do you write with? I'm seventy five now....\n",
      "üìù Summary preview: This patient has had chronic lower back pain for 10 days, which is worsening, with radiating pain to both legs. His symptoms began after he was diagnosed with aortic stenosis, which may be contributing to his pain.\n",
      "\n",
      "üìù Prompt length: 2347 characters\n",
      "\n",
      "ü§ñ MODEL RESPONSE:\n",
      "\n",
      "\n",
      "Please rate the generated summary against the source patient-doctor conversation. \n",
      "\n",
      "Hallucination: 2 (The summary mentions aortic stenosis, which is not mentioned in the conversation)\n",
      "Factual: 3 (The summary is mostly accurate, but it does not mention the patient's age, the initial date of the back pain, or the patient's ability to walk)\n",
      "Complete: 2 (The summary is missing key information, such as the patient's age, the initial date of\n",
      "\n",
      "üìä EXTRACTED SCORES:\n",
      "Hallucination: 2\n",
      "Factual: 3\n",
      "Complete: 2\n",
      "Coherent: Not found\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Does the test look good? Run full evaluation? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running full MedAlpaca SOAP evaluation...\n",
      "üîÑ Loading Llama 3 8B model as judge...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "004f5ba1d9cb483eb6be0ff29a3d0ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Loaded 100 MedAlpaca-7B generated SOAP summaries\n",
      "üìã Columns: ['sample_id', 'original_input', 'generated_summary', 'target_summary', 'summary_token_count', 'input_tokens', 'target_tokens']\n",
      "üîç Evaluating MedAlpaca SOAP summaries for hallucinations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/100 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:   1%|          | 1/100 [00:04<07:10,  4.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:   2%|‚ñè         | 2/100 [00:07<06:23,  3.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:   3%|‚ñé         | 3/100 [00:11<05:46,  3.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:   4%|‚ñç         | 4/100 [00:17<07:15,  4.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:   5%|‚ñå         | 5/100 [00:23<08:01,  5.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:   6%|‚ñå         | 6/100 [00:29<08:26,  5.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:   7%|‚ñã         | 7/100 [00:34<08:10,  5.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:   8%|‚ñä         | 8/100 [00:36<06:24,  4.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:   9%|‚ñâ         | 9/100 [00:38<05:41,  3.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  10%|‚ñà         | 10/100 [00:41<05:14,  3.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  11%|‚ñà         | 11/100 [00:47<06:19,  4.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  12%|‚ñà‚ñè        | 12/100 [00:53<07:01,  4.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  13%|‚ñà‚ñé        | 13/100 [00:56<06:12,  4.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  14%|‚ñà‚ñç        | 14/100 [00:59<05:26,  3.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  15%|‚ñà‚ñå        | 15/100 [01:01<04:36,  3.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  16%|‚ñà‚ñå        | 16/100 [01:04<04:36,  3.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  17%|‚ñà‚ñã        | 17/100 [01:10<05:40,  4.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  18%|‚ñà‚ñä        | 18/100 [01:14<05:24,  3.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  19%|‚ñà‚ñâ        | 19/100 [01:20<06:14,  4.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  20%|‚ñà‚ñà        | 20/100 [01:22<05:04,  3.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  21%|‚ñà‚ñà        | 21/100 [01:28<05:57,  4.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  22%|‚ñà‚ñà‚ñè       | 22/100 [01:31<05:14,  4.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  23%|‚ñà‚ñà‚ñé       | 23/100 [01:35<05:10,  4.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  24%|‚ñà‚ñà‚ñç       | 24/100 [01:41<05:49,  4.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  25%|‚ñà‚ñà‚ñå       | 25/100 [01:44<05:10,  4.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  26%|‚ñà‚ñà‚ñå       | 26/100 [01:50<05:47,  4.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  27%|‚ñà‚ñà‚ñã       | 27/100 [01:56<06:11,  5.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  28%|‚ñà‚ñà‚ñä       | 28/100 [02:02<06:25,  5.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  29%|‚ñà‚ñà‚ñâ       | 29/100 [02:08<06:34,  5.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  30%|‚ñà‚ñà‚ñà       | 30/100 [02:13<06:06,  5.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  31%|‚ñà‚ñà‚ñà       | 31/100 [02:16<05:14,  4.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/100 [02:22<05:38,  4.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [02:28<05:54,  5.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  34%|‚ñà‚ñà‚ñà‚ñç      | 34/100 [02:34<06:02,  5.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  35%|‚ñà‚ñà‚ñà‚ñå      | 35/100 [02:37<05:10,  4.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/100 [02:40<04:36,  4.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  37%|‚ñà‚ñà‚ñà‚ñã      | 37/100 [02:43<04:03,  3.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/100 [02:47<03:57,  3.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  39%|‚ñà‚ñà‚ñà‚ñâ      | 39/100 [02:53<04:33,  4.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  40%|‚ñà‚ñà‚ñà‚ñà      | 40/100 [02:59<04:56,  4.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [03:02<04:22,  4.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/100 [03:05<03:55,  4.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/100 [03:11<04:24,  4.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 44/100 [03:17<04:42,  5.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 45/100 [03:20<04:10,  4.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/100 [03:24<03:58,  4.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/100 [03:28<03:35,  4.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/100 [03:34<04:01,  4.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [03:39<04:06,  4.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 50/100 [03:45<04:19,  5.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/100 [03:51<04:25,  5.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/100 [03:54<03:47,  4.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 53/100 [03:57<03:21,  4.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 54/100 [04:03<03:40,  4.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 55/100 [04:06<03:03,  4.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/100 [04:12<03:24,  4.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [04:18<03:39,  5.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 58/100 [04:23<03:28,  4.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 59/100 [04:25<02:55,  4.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 60/100 [04:30<02:57,  4.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/100 [04:31<02:16,  3.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/100 [04:35<02:09,  3.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 63/100 [04:41<02:34,  4.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 64/100 [04:46<02:44,  4.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [04:49<02:24,  4.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/100 [04:53<02:20,  4.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/100 [04:59<02:34,  4.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 68/100 [05:04<02:32,  4.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 69/100 [05:09<02:26,  4.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 70/100 [05:15<02:33,  5.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/100 [05:21<02:35,  5.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/100 [05:22<01:55,  4.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [05:25<01:40,  3.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 74/100 [05:28<01:31,  3.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 75/100 [05:32<01:31,  3.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/100 [05:36<01:34,  3.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 77/100 [05:42<01:44,  4.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 78/100 [05:46<01:33,  4.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 79/100 [05:52<01:40,  4.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 80/100 [05:58<01:42,  5.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [06:04<01:42,  5.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 82/100 [06:10<01:40,  5.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 83/100 [06:16<01:36,  5.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 84/100 [06:22<01:32,  5.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 85/100 [06:28<01:27,  5.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/100 [06:32<01:17,  5.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 87/100 [06:38<01:13,  5.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 88/100 [06:44<01:08,  5.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [06:50<01:01,  5.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 90/100 [06:52<00:45,  4.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/100 [06:58<00:45,  5.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 92/100 [07:02<00:38,  4.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 93/100 [07:05<00:28,  4.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 94/100 [07:08<00:23,  3.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 95/100 [07:14<00:22,  4.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/100 [07:20<00:19,  4.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [07:26<00:15,  5.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 98/100 [07:32<00:10,  5.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 99/100 [07:34<00:04,  4.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [07:40<00:00,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä MEDALPACA-7B SOAP HALLUCINATION EVALUATION RESULTS:\n",
      "======================================================================\n",
      "Metric                    Mean ¬± Std      Min    Max    Perfect Scores\n",
      "----------------------------------------------------------------------\n",
      "Hallucination (1-5)       2.13 ¬± 0.44   1.0    4.0    3/100\n",
      "Factual Consistency (1-5) 3.86 ¬± 0.38   2.0    4.0    0/100\n",
      "Completeness (1-5)        3.07 ¬± 0.38   2.0    5.0    2/100\n",
      "Coherence (1-5)           4.68 ¬± 0.66   3.0    5.0    79/100\n",
      "\n",
      "üìã BASELINE QUALITY INSIGHTS:\n",
      "‚Ä¢ High hallucination (‚â•4): 1/100 (1.0%)\n",
      "‚Ä¢ Low factual consistency (‚â§2): 1/100 (1.0%)\n",
      "‚Ä¢ Good completeness (‚â•4): 7/100 (7.0%)\n",
      "‚Ä¢ Good coherence (‚â•4): 89/100 (89.0%)\n",
      "\n",
      "üìä FOR BASELINE TABLE:\n",
      "Hallucination: 2.13 ¬± 0.44\n",
      "Factual Consistency: 3.86 ¬± 0.38\n",
      "Completeness: 3.07 ¬± 0.38\n",
      "Coherence: 4.68 ¬± 0.66\n",
      "\n",
      "üíæ Results saved to: medalpaca_soap_judge_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%run llm_as_a_judge_medalpaca.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667b4880-2e15-4a0e-bfa6-66fb29d0670a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
