{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89f3f2fc-f5c1-4748-9860-be36fba8de63",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers huggingface_hub\n",
    "!pip install -q --upgrade accelerate\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95433f7c-07db-4f7a-870b-f16581dae789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/fsspec/registry.py:294: UserWarning: Your installed version of s3fs is very old and known to cause\n",
      "severe performance issues, see also https://github.com/dask/dask/issues/10276\n",
      "\n",
      "To fix, you should specify a lower version bound on s3fs, or\n",
      "update the current installation.\n",
      "\n",
      "  warnings.warn(s3_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good afternoon, champ, how you holding up? Goo...</td>\n",
       "      <td>Subjective:\\n- Symptoms: Lower back pain, radi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What brings you in here today? Hi, I'm um, I'm...</td>\n",
       "      <td>Subjective:\\n- Presenting with dry cough for 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Do you have any known allergies to medications...</td>\n",
       "      <td>Subjective:\\n- No known allergies to medicatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How may I help you today? Yeah I've had, a fev...</td>\n",
       "      <td>Subjective:\\n- Fever and dry cough started 4 d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It sounds like that you're experiencing some c...</td>\n",
       "      <td>Subjective:\\n- Presenting with chest pain for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>What brings you in? Hi. Uh, I've just had this...</td>\n",
       "      <td>Subjective:\\n- Cough for the past week\\n- Pers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Good morning, ma'am. Oh, good morning, doctor....</td>\n",
       "      <td>Subjective:\\n- Symptoms: Difficulty using stai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Are you aware of any medical problems in your ...</td>\n",
       "      <td>Subjective:\\n- No medical problems reported in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>What's going on with you? What brings you here...</td>\n",
       "      <td>Subjective:\\n- Symptoms: Loose watery stools\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>How are you doing today, ma'am? I'm doing just...</td>\n",
       "      <td>Subjective:\\n- Symptoms: Issues with knees.\\n-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                input  \\\n",
       "0   Good afternoon, champ, how you holding up? Goo...   \n",
       "1   What brings you in here today? Hi, I'm um, I'm...   \n",
       "2   Do you have any known allergies to medications...   \n",
       "3   How may I help you today? Yeah I've had, a fev...   \n",
       "4   It sounds like that you're experiencing some c...   \n",
       "..                                                ...   \n",
       "94  What brings you in? Hi. Uh, I've just had this...   \n",
       "95  Good morning, ma'am. Oh, good morning, doctor....   \n",
       "96  Are you aware of any medical problems in your ...   \n",
       "97  What's going on with you? What brings you here...   \n",
       "98  How are you doing today, ma'am? I'm doing just...   \n",
       "\n",
       "                                               output  \n",
       "0   Subjective:\\n- Symptoms: Lower back pain, radi...  \n",
       "1   Subjective:\\n- Presenting with dry cough for 1...  \n",
       "2   Subjective:\\n- No known allergies to medicatio...  \n",
       "3   Subjective:\\n- Fever and dry cough started 4 d...  \n",
       "4   Subjective:\\n- Presenting with chest pain for ...  \n",
       "..                                                ...  \n",
       "94  Subjective:\\n- Cough for the past week\\n- Pers...  \n",
       "95  Subjective:\\n- Symptoms: Difficulty using stai...  \n",
       "96  Subjective:\\n- No medical problems reported in...  \n",
       "97  Subjective:\\n- Symptoms: Loose watery stools\\n...  \n",
       "98  Subjective:\\n- Symptoms: Issues with knees.\\n-...  \n",
       "\n",
       "[99 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the bucket and file names\n",
    "bucket_name = 'agentsum'  # Replace with your bucket name\n",
    "soap = f's3://{bucket_name}/sample_summary.csv'\n",
    "\n",
    "# Load the files\n",
    "soap = pd.read_csv(soap)\n",
    "\n",
    "# Display the data\n",
    "soap.head(99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39345044-9891-4b23-b28f-5455de2e90f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your Hugging Face token:  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'user', 'id': '6644f24fe7ae8316ebf3fee4', 'name': 'LizaPiya', 'fullname': 'Fahmida Liza Piya', 'email': 'lizapiya@udel.edu', 'emailVerified': True, 'canPay': False, 'periodEnd': None, 'isPro': False, 'avatarUrl': '/avatars/d43d60b3eba464c3f9b44c34e43b64d6.svg', 'orgs': [], 'auth': {'type': 'access_token', 'accessToken': {'displayName': 'Clinical Note LLama', 'role': 'write', 'createdAt': '2024-06-03T19:29:07.142Z'}}}\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from huggingface_hub import whoami\n",
    "import getpass\n",
    "\n",
    "# Prompt the user for the Hugging Face token at runtime\n",
    "hf_token = getpass.getpass(\"Enter your Hugging Face token: \")\n",
    "\n",
    "# Log in using the provided token\n",
    "login(token=hf_token)\n",
    "\n",
    "\n",
    "print(whoami(token=hf_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d781a0fb-2834-4e08-ad2f-d30eda249e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a38c878f-f311-4eed-b57f-d1c8e3cfcde5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196c07f63e654730abf42a095a0920df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/141k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ee1d3d403b4b3b95fd83c4c446a7fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de7575694a2641dc805b1f883c0014e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf1d63c04bd04ffdaab917a5585f89b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8896b935ea4b468fbd6bdac483647ca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "777eb4ee24a145018e843c40077e9971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca45cd321c8c46408e641671e355350f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a32554ae934e7689670d8b2369de75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c8217a18f9b430c94cf9685a85e174a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6ac8053c78448f6a532a61c08a58b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09ac4e568d01487caf5010f29a8455a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d6d10cbb4664b46aefad8ba5b432195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and processing SOAP dataset...\n",
      "Total samples to process: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [11:25<00:00,  6.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All results saved to 'mistral_7b_summaries_soap_dataset.csv'\n",
      "\n",
      "Dataset Statistics:\n",
      "Total samples processed: 100\n",
      "Successful generations: 100\n",
      "Failed generations: 0\n",
      "\n",
      "Average token counts:\n",
      "Input tokens: 486.7\n",
      "Target tokens: 191.3\n",
      "Generated summary tokens: 348.2\n",
      "\n",
      "Sample result:\n",
      "Input (first 200 chars): Good afternoon, champ, how you holding up? Good afternoon, Doctor, I have a lot of lower back pain. Oh no, before we begin, how old are you, sir and which hand do you write with? I'm seventy five now....\n",
      "Generated summary: Subjective:\n",
      "- Patient is a 75-year-old male who has been experiencing lower back pain for 10 days, with radiating pain down both legs for the past 6 days, causing severe weakness and difficulty walking. The pain started on December 3, 1995. The patient has seen two doctors and received anti-inflammatory medication, but the symptoms have worsened. There is leg numbness since December 11.\n",
      "\n",
      "Objective:\n",
      "- No objective data is provided in the dialogue.\n",
      "\n",
      "Assessment:\n",
      "- The patient may be experiencing symptoms of a herniated disc or spinal stenosis, given the radiating lower back pain, leg weakness, and numbness. The absence of findings on imaging studies could suggest a misdiagnosis or an incomplete herniation.\n",
      "\n",
      "Plan:\n",
      "- Further diagnostic tests such as MRI or myelogram\n",
      "Target summary: Subjective:\n",
      "- Symptoms: Lower back pain, radiating pain down the right leg, then the left leg.\n",
      "- Severity: Severe pain, described as \"I could barely walk\" and \"the pain was so severe.\"\n",
      "- Duration: Lower back pain for about ten days; radiating pain started three days after back pain began (right leg), then left leg three days later.\n",
      "- Associated symptoms: Weakness in the legs, leg numbness since December eleventh.\n",
      "- Relevant medical history: Saw local physician on December eleventh; received anti-inflammatory medications which were ineffective.\n",
      "- Family history: Not mentioned.\n",
      "- Allergies: Not mentioned.\n",
      "- Other concerns: Patient expressed difficulty walking and concerns about the severity of their symptoms.\n",
      "\n",
      "Objective:\n",
      "- Physical examination findings: Not provided.\n",
      "- Diagnostic test results: CT and X-rays of the lower back performed by the orthopedist, but no abnormalities were noted.\n",
      "- Vital signs: Not provided.\n",
      "\n",
      "Assessment:\n",
      "- Diagnoses: Not explicitly stated, but indicated severe lower back pain with radiculopathy.\n",
      "- Differential diagnoses: Not provided.\n",
      "- Clinical impressions: Not explicitly stated.\n",
      "\n",
      "Plan:\n",
      "- Follow up at U I H C as recommended by the orthopedist. \n",
      "- No specific medications, treatments, or further tests mentioned in the conversation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load model and tokenizer - CHANGE 1: Model name (kept the same)\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Medical dialogue summarization function - CHANGE 2: Updated for SOAP dialogue\n",
    "def generate_summary(dialogue):\n",
    "    # CHANGE 3: Updated prompt for medical dialogue and SOAP format\n",
    "    prompt = f\"\"\"You are a medical expert. Analyze the following patient-doctor dialogue and provide a concise medical summary in SOAP format (Subjective, Objective, Assessment, Plan):\\n\\n{dialogue}\\n\\nSummary:\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,  # CHANGE 4: Adjusted for dialogue summarization\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True).replace(prompt, \"\").strip()\n",
    "\n",
    "# Apply to SOAP dataset - CHANGE 5: Updated function name and structure\n",
    "def process_soap_data(df):\n",
    "    results = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Generating summaries\"):\n",
    "        try:\n",
    "            summary = generate_summary(row['input'])\n",
    "            summary_tokens = len(tokenizer.encode(summary))\n",
    "            input_tokens = len(tokenizer.encode(row['input']))\n",
    "            target_tokens = len(tokenizer.encode(row['output']))  # CHANGE 6: 'output' instead of 'target'\n",
    "            \n",
    "            results.append({\n",
    "                'sample_id': idx,  # CHANGE 7: sample_id instead of note_id\n",
    "                'original_input': row['input'],\n",
    "                'generated_summary': summary,\n",
    "                'target_summary': row['output'],  # CHANGE 8: 'output' column from SOAP\n",
    "                'summary_token_count': summary_tokens,\n",
    "                'input_tokens': input_tokens,\n",
    "                'target_tokens': target_tokens\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample {idx}: {str(e)}\")\n",
    "            results.append({\n",
    "                'sample_id': idx,\n",
    "                'original_input': row['input'],\n",
    "                'generated_summary': \"ERROR: Could not generate summary\",\n",
    "                'target_summary': row['output'],\n",
    "                'summary_token_count': 0,\n",
    "                'input_tokens': len(tokenizer.encode(row['input'])) if pd.notna(row['input']) else 0,\n",
    "                'target_tokens': len(tokenizer.encode(row['output'])) if pd.notna(row['output']) else 0\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run the pipeline - CHANGE 9: Updated for SOAP dataset\n",
    "print(\"Loading and processing SOAP dataset...\")\n",
    "print(f\"Total samples to process: {len(soap)}\")  # CHANGE 10: 'soap' instead of 'mimic_iv_bhc_100'\n",
    "\n",
    "full_results = process_soap_data(soap)  # CHANGE 11: Updated function call\n",
    "\n",
    "# CHANGE 12: Update output filename for SOAP\n",
    "full_results.to_csv('mistral_7b_summaries_soap_dataset.csv', index=False)\n",
    "print(f\"\\nAll results saved to 'mistral_7b_summaries_soap_dataset.csv'\")\n",
    "\n",
    "# Display some basic statistics\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"Total samples processed: {len(full_results)}\")\n",
    "print(f\"Successful generations: {len(full_results[full_results['generated_summary'] != 'ERROR: Could not generate summary'])}\")\n",
    "print(f\"Failed generations: {len(full_results[full_results['generated_summary'] == 'ERROR: Could not generate summary'])}\")\n",
    "\n",
    "# Display average token counts\n",
    "successful_results = full_results[full_results['generated_summary'] != 'ERROR: Could not generate summary']\n",
    "if len(successful_results) > 0:\n",
    "    print(f\"\\nAverage token counts:\")\n",
    "    print(f\"Input tokens: {successful_results['input_tokens'].mean():.1f}\")\n",
    "    print(f\"Target tokens: {successful_results['target_tokens'].mean():.1f}\")\n",
    "    print(f\"Generated summary tokens: {successful_results['summary_token_count'].mean():.1f}\")\n",
    "\n",
    "# Show a sample result\n",
    "if len(successful_results) > 0:\n",
    "    print(f\"\\nSample result:\")\n",
    "    sample = successful_results.iloc[0]\n",
    "    print(f\"Input (first 200 chars): {sample['original_input'][:200]}...\")\n",
    "    print(f\"Generated summary: {sample['generated_summary']}\")\n",
    "    print(f\"Target summary: {sample['target_summary']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b2b708-bbcc-432c-8636-3a2e188c82d9",
   "metadata": {},
   "source": [
    "### Traditional MetricsÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9a41ef3-4c58-409c-8012-dfdae1db7940",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q nltk bert-score\n",
    "!pip install -q rouge-metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5790f320-9806-45a7-b6c0-daec9cb9e0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Run directly\n",
    "%run Soap_Mistral.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e90485d-5492-4531-8645-18436b97a2d2",
   "metadata": {},
   "source": [
    "### llm as a judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "411ee256-0501-4ccd-807c-84f3c4324db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing single Mistral SOAP sample...\n",
      "ðŸ§ª Testing single Mistral SOAP sample evaluation...\n",
      "Testing sample 0 from 100 total samples\n",
      "Sample ID: 0\n",
      "Summary token count: 200\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94d67a38c474bd28cf56811ec17bbdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“„ Source length: 1701 characters\n",
      "ðŸ“ Summary length: 771 characters\n",
      "ðŸ“„ Source preview: Good afternoon, champ, how you holding up? Good afternoon, Doctor, I have a lot of lower back pain. Oh no, before we begin, how old are you, sir and which hand do you write with? I'm seventy five now....\n",
      "ðŸ“ Summary preview: Subjective:\n",
      "- Patient is a 75-year-old male who has been experiencing lower back pain for 10 days, with radiating pain down both legs for the past 6 days, causing severe weakness and difficulty walking. The pain started on December 3, 1995. The patient has seen two doctors and received anti-inflammatory medication, but the symptoms have worsened. There is leg numbness since December 11.\n",
      "\n",
      "Objective:\n",
      "- No objective data is provided in the dialogue.\n",
      "\n",
      "Assessment:\n",
      "- The patient may be experiencing symptoms of a herniated disc or spinal stenosis, given the radiating lower back pain, leg weakness, and numbness. The absence of findings on imaging studies could suggest a misdiagnosis or an incomplete herniation.\n",
      "\n",
      "Plan:\n",
      "- Further diagnostic tests such as MRI or myelogram\n",
      "\n",
      "ðŸ“ Prompt length: 2904 characters\n",
      "\n",
      "ðŸ¤– MODEL RESPONSE:\n",
      "\n",
      "\n",
      "Please rate the generated SOAP summary against the source patient-doctor conversation. \n",
      "\n",
      "Note: The generated SOAP summary is a summary of the patient-doctor conversation, not a direct transcription. It is intended to be a concise and organized summary of the patient's symptoms, assessment, and plan. \n",
      "\n",
      "Please provide a brief explanation for each rating. \n",
      "\n",
      "Thank you! \n",
      "\n",
      "Best regards, \n",
      "[Your Name]  | Read 1,000+ times | 1 Comment\n",
      "\n",
      "I would rate the generated\n",
      "\n",
      "ðŸ“Š EXTRACTED SCORES:\n",
      "Hallucination: Not found\n",
      "Factual: Not found\n",
      "Complete: Not found\n",
      "Coherent: Not found\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Does the test look good? Run full evaluation? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running full Mistral SOAP evaluation...\n",
      "ðŸ”„ Loading Llama 3 8B model as judge...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36836c0b813145a8af8a514212aa7302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Loaded 100 Mistral-7B generated SOAP summaries\n",
      "ðŸ“‹ Columns: ['sample_id', 'original_input', 'generated_summary', 'target_summary', 'summary_token_count', 'input_tokens', 'target_tokens']\n",
      "ðŸ” Evaluating Mistral SOAP summaries for hallucinations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/100 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:   1%|          | 1/100 [00:03<06:24,  3.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:   2%|â–         | 2/100 [00:06<04:49,  2.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:   3%|â–Ž         | 3/100 [00:11<06:52,  4.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:   4%|â–         | 4/100 [00:18<08:01,  5.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:   5%|â–Œ         | 5/100 [00:23<08:20,  5.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:   6%|â–Œ         | 6/100 [00:30<08:43,  5.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:   7%|â–‹         | 7/100 [00:36<08:56,  5.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:   8%|â–Š         | 8/100 [00:42<09:02,  5.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:   9%|â–‰         | 9/100 [00:48<09:04,  5.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  10%|â–ˆ         | 10/100 [00:49<06:46,  4.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  11%|â–ˆ         | 11/100 [00:55<07:24,  4.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  12%|â–ˆâ–        | 12/100 [01:02<07:50,  5.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  13%|â–ˆâ–Ž        | 13/100 [01:08<08:06,  5.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  14%|â–ˆâ–        | 14/100 [01:10<06:39,  4.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  15%|â–ˆâ–Œ        | 15/100 [01:12<05:32,  3.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  16%|â–ˆâ–Œ        | 16/100 [01:15<04:56,  3.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  17%|â–ˆâ–‹        | 17/100 [01:21<05:58,  4.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  18%|â–ˆâ–Š        | 18/100 [01:27<06:38,  4.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  19%|â–ˆâ–‰        | 19/100 [01:30<05:35,  4.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  20%|â–ˆâ–ˆ        | 20/100 [01:36<06:19,  4.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  21%|â–ˆâ–ˆ        | 21/100 [01:39<05:34,  4.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  22%|â–ˆâ–ˆâ–       | 22/100 [01:45<06:14,  4.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  23%|â–ˆâ–ˆâ–Ž       | 23/100 [01:48<05:34,  4.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  24%|â–ˆâ–ˆâ–       | 24/100 [01:54<06:05,  4.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  25%|â–ˆâ–ˆâ–Œ       | 25/100 [02:00<06:30,  5.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  26%|â–ˆâ–ˆâ–Œ       | 26/100 [02:06<06:45,  5.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  27%|â–ˆâ–ˆâ–‹       | 27/100 [02:13<06:54,  5.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  28%|â–ˆâ–ˆâ–Š       | 28/100 [02:19<06:58,  5.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  29%|â–ˆâ–ˆâ–‰       | 29/100 [02:25<07:00,  5.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  30%|â–ˆâ–ˆâ–ˆ       | 30/100 [02:30<06:28,  5.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  31%|â–ˆâ–ˆâ–ˆ       | 31/100 [02:33<05:35,  4.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [02:37<05:12,  4.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [02:43<05:38,  5.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [02:49<05:55,  5.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [02:55<06:04,  5.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [02:58<05:01,  4.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [03:04<05:23,  5.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [03:06<04:16,  4.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [03:09<03:47,  3.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [03:15<04:25,  4.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [03:18<03:58,  4.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [03:20<03:28,  3.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/100 [03:26<04:08,  4.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [03:30<03:43,  3.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [03:36<04:14,  4.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [03:38<03:34,  3.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [03:44<04:04,  4.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [03:47<03:24,  3.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [03:53<03:53,  4.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [03:56<03:29,  4.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [04:02<03:53,  4.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [04:04<03:11,  3.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [04:07<02:50,  3.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [04:11<02:48,  3.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [04:17<03:17,  4.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [04:23<03:36,  4.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [04:26<03:07,  4.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [04:28<02:34,  3.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [04:31<02:20,  3.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [04:34<02:10,  3.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [04:40<02:41,  4.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [04:43<02:21,  3.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [04:48<02:31,  4.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [04:54<02:49,  4.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [04:59<02:46,  4.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [05:05<02:56,  5.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [05:08<02:25,  4.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [05:11<02:11,  4.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [05:17<02:26,  4.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [05:23<02:33,  5.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [05:29<02:37,  5.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [05:36<02:39,  5.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [05:42<02:37,  5.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [05:48<02:34,  5.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [05:54<02:29,  5.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [06:00<02:25,  6.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [06:04<02:01,  5.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [06:07<01:40,  4.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [06:13<01:45,  5.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [06:18<01:43,  5.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [06:24<01:43,  5.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [06:31<01:41,  5.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [06:34<01:22,  4.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [06:38<01:13,  4.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [06:40<01:00,  4.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [06:42<00:45,  3.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [06:48<00:53,  4.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [06:50<00:43,  3.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [06:56<00:48,  4.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [06:59<00:38,  3.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [07:02<00:32,  3.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [07:09<00:35,  4.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [07:14<00:33,  4.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [07:20<00:31,  5.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 95/100 [07:27<00:27,  5.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 96/100 [07:32<00:22,  5.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [07:38<00:17,  5.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 98/100 [07:41<00:09,  4.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99/100 [07:47<00:05,  5.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [07:53<00:00,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š MISTRAL-7B SOAP HALLUCINATION EVALUATION RESULTS:\n",
      "======================================================================\n",
      "Metric                    Mean Â± Std      Min    Max    Perfect Scores\n",
      "----------------------------------------------------------------------\n",
      "Hallucination (1-5)       2.23 Â± 0.58   1.0    3.0    8/100\n",
      "Factual Consistency (1-5) 3.74 Â± 0.50   2.0    5.0    2/100\n",
      "Completeness (1-5)        2.99 Â± 0.30   2.0    5.0    1/100\n",
      "Coherence (1-5)           4.05 Â± 0.98   3.0    5.0    50/100\n",
      "\n",
      "ðŸ“‹ BASELINE QUALITY INSIGHTS:\n",
      "â€¢ High hallucination (â‰¥4): 0/100 (0.0%)\n",
      "â€¢ Low factual consistency (â‰¤2): 1/100 (1.0%)\n",
      "â€¢ Good completeness (â‰¥4): 2/100 (2.0%)\n",
      "â€¢ Good coherence (â‰¥4): 55/100 (55.0%)\n",
      "\n",
      "ðŸ“Š FOR BASELINE TABLE:\n",
      "Hallucination: 2.23 Â± 0.58\n",
      "Factual Consistency: 3.74 Â± 0.50\n",
      "Completeness: 2.99 Â± 0.30\n",
      "Coherence: 4.05 Â± 0.98\n",
      "\n",
      "ðŸ’¾ Results saved to: mistral_soap_judge_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%run llm_as_a_judge_mistral.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689fad45-bdce-4a6e-9feb-d398cb272291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
